import os
import multiprocessing
from datetime import datetime
from pathlib import Path
import argparse
from pathlib import Path
import glob
import sys
import subprocess


## auto_detect_workers
## create_default_log(prefix="log", sample_id=None, outdir=None)
## validate_path( must_exist=True, must_be_file=False, must_be_dir=False, create_if_missing=False, must_not_be_empty=False,
    ## allowed_suffixes=None, dir_must_have_files=False, dir_require_nonempty_files=None, dir_nonempty_suffixes=None,
    # dir_forbid_empty_files=False)
    
## detect_total_memory_gb()

## validate_plink_prefix
## safe_thread_count(requested_threads, gb_per_thread = 20)
## run_cmd(cmd)



# ------------------------------------------------------------
# Reusable command runner with clean failure messages
# ------------------------------------------------------------
def run_cmd(cmd, check=True, shell=None, capture_output=True, text=True):
    """
    Simple, generalised subprocess runner.
    - If shell is not provided:
        * list command  -> shell=False
        * string command -> shell=True
    """
    # Auto-detect shell if not specified
    if shell is None:
        shell = isinstance(cmd, str)

    try:
        result = subprocess.run(
            cmd,
            check=check,
            shell=shell,
            capture_output=capture_output,
            text=text
        )
        return result

    except FileNotFoundError:
        raise FileNotFoundError(
            f"❌ Executable not found: {cmd}\n"
            "Ensure it is installed and available in your PATH."
        )

    except subprocess.CalledProcessError as e:
        raise RuntimeError(
            f"❌ Command failed with exit code {e.returncode}:\n"
            f"   {cmd}\n"
            "--------------- stderr ---------------\n"
            f"{e.stderr}"
        ) from e



def infer_mount_root(*paths):
    """
    Infer the deepest common parent directory across all input paths.
    """
    resolved = [str(Path(p).expanduser().resolve()) for p in paths]
    common = os.path.commonpath(resolved)
    return Path(common)




def auto_detect_workers():
    """
    Universal CPU auto-detection.
    - Works on macOS, Linux, Windows, HPC (SLURM), Docker/Cgroups.
    - Always returns max(1, available_cpus - 1) for safe use.
    """
    # ----------------------------------------------------
    # 1) Check SLURM limits (HPC clusters)
    # ----------------------------------------------------
    slurm_vars = [
        "SLURM_CPUS_PER_TASK",
        "SLURM_CPUS_ON_NODE",
        "SLURM_JOB_CPUS_PER_NODE"
    ]
    for var in slurm_vars:
        val = os.environ.get(var)
        if val and val.isdigit():
            cpus = int(val)
            return max(1, cpus - 1)
    # ----------------------------------------------------
    # 2) Check cgroup CPU quota (Docker / Kubernetes)
    # ----------------------------------------------------
    try:
        cfs_quota = int(open("/sys/fs/cgroup/cpu/cpu.cfs_quota_us").read())
        cfs_period = int(open("/sys/fs/cgroup/cpu/cpu.cfs_period_us").read())
        if cfs_quota > 0:
            cpus = cfs_quota // cfs_period
            if cpus > 0:
                return max(1, cpus - 2)
    except:
        pass
    # ----------------------------------------------------
    # 3) Fallback to the OS-reported CPU count
    # ----------------------------------------------------
    try:
        cpus = multiprocessing.cpu_count()
        return max(1, cpus - 1)
    except:
        return 1  # absolute fallback



def safe_thread_count(requested_threads, gb_per_thread = 20):
    """
    Ensure the number of threads does not exceed available memory.
    Example: If each thread needs 20GB, auto-adjust threads based on RAM.
    Parameters
    ----------
    requested_threads : int
        Number of threads requested by user.
    gb_per_thread : int
        Minimum GB of RAM required per thread.
    Returns
    -------
    int
        Safe number of threads based on system memory.
    """
    try:
        import psutil
        total_ram_gb = psutil.virtual_memory().total / (1024**3)
    except ImportError:
        print("⚠️ psutil not installed → falling back to requested threads.")
        return requested_threads
    # Max threads based on memory
    max_threads = int(total_ram_gb // gb_per_thread)
    if max_threads < 1:
        print(f"❌ Not enough RAM: {total_ram_gb:.1f} GB available, "
              f"but require ≥ {gb_per_thread} GB per thread.")
        return 1
    if requested_threads > max_threads:
        print(f"⚠️ Reducing threads from {requested_threads} → {max_threads} "
              f"(RAM available: {total_ram_gb:.1f} GB; {gb_per_thread} GB/thread)")
        return max_threads
    
    print(f"✔ Using {requested_threads} threads "
          f"(RAM available: {total_ram_gb:.1f} GB; {gb_per_thread} GB/thread)")
    return requested_threads




def create_default_log(prefix="log", sample_id=None, outdir=None):
    """
    Create a unified default log filename for any module.

    Parameters
    ----------
    prefix : str
        Module name prefix (e.g., 'flames', 'susie', 'magma').
    sample_id : str or None
        Optional sample or phenotype name (e.g., 'PGC3_SCZ').
    outdir : str or Path or None
        Output directory where log should be placed. Default: current working directory.

    Returns
    -------
    str : Full path to log file.
    """
    if outdir is None:
        outdir = Path.cwd()
    else:
        outdir = Path(outdir).expanduser().resolve()

    outdir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if sample_id:
        filename = f"{prefix}_{sample_id}_{timestamp}.log"
    else:
        filename = f"{prefix}_{timestamp}.log"

    return str(outdir / filename)


def validate_path(
    must_exist=False,
    must_be_file=False,
    must_be_dir=False,
    create_if_missing=False,
    must_not_be_empty=False,
    allowed_suffixes=None,

    # ---- NEW DIRECTORY RULES ----
    dir_must_have_files=False,
    dir_require_nonempty_files=None,        
    dir_nonempty_suffixes=None,            
    dir_forbid_empty_files=False,          
):
    """
    Generalized path validator with expanded directory/file checks.
    """

    def _validator(path_str):
        p = Path(path_str)

        # ------------------------------------------------------------
        # CREATE DIRECTORY IF MISSING
        # ------------------------------------------------------------
        if create_if_missing:
            if p.exists() and not p.is_dir():
                raise argparse.ArgumentTypeError(f"❌ Expected a directory but got file: {p}")
            if not p.exists():
                try:
                    p.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    raise argparse.ArgumentTypeError(f"❌ Cannot create directory {p}: {e}")
            return p

        # ------------------------------------------------------------
        # BASIC EXISTENCE + TYPE CHECKS
        # ------------------------------------------------------------
        if must_exist and not p.exists():
            raise argparse.ArgumentTypeError(f"❌ Path does not exist: {p}")

        if must_be_file and not p.is_file():
            raise argparse.ArgumentTypeError(f"❌ Expected a file but got: {p}")

        if must_be_dir and not p.is_dir():
            raise argparse.ArgumentTypeError(f"❌ Expected a directory but got: {p}")

        # ------------------------------------------------------------
        # FILE-SPECIFIC CHECKS
        # ------------------------------------------------------------
        if p.is_file():
            if must_not_be_empty and p.stat().st_size == 0:
                raise argparse.ArgumentTypeError(f"❌ File is empty: {p}")

            if allowed_suffixes:
                suffix = "".join(p.suffixes)
                allowed = {s if s.startswith(".") else f".{s}" for s in allowed_suffixes}
                if suffix not in allowed:
                    raise argparse.ArgumentTypeError(
                        f"❌ Invalid suffix '{suffix}' for {p}. Allowed: {sorted(allowed)}"
                    )
            return p

        # ------------------------------------------------------------
        # DIRECTORY-SPECIFIC CHECKS
        # ------------------------------------------------------------
        if p.is_dir():
            files = [f for f in p.iterdir() if f.is_file()]

            # ---- Require directory not to be empty ----
            if dir_must_have_files and len(files) == 0:
                raise argparse.ArgumentTypeError(f"❌ Directory contains no files: {p}")

            # ---- STRICT MODE: no empty files allowed ----
            if dir_forbid_empty_files:
                empty_files = [f for f in files if f.stat().st_size == 0]
                if empty_files:
                    raise argparse.ArgumentTypeError(
                        "❌ Directory contains empty files:\n" +
                        "\n".join(f"   - {f}" for f in empty_files)
                    )

            # ---- NEW RULE: At least N non-empty files ----
            if dir_require_nonempty_files is not None:
                # Decide which files count
                if dir_nonempty_suffixes:
                    allowed = {s if s.startswith(".") else f".{s}"
                               for s in dir_nonempty_suffixes}
                    relevant = [
                        f for f in files
                        if "".join(f.suffixes) in allowed
                    ]
                else:
                    relevant = files

                nonempty = [f for f in relevant if f.stat().st_size > 0]

                if len(nonempty) < dir_require_nonempty_files:
                    raise argparse.ArgumentTypeError(
                        f"❌ Directory requires at least "
                        f"{dir_require_nonempty_files} non-empty files "
                        f"(found {len(nonempty)}).\nDirectory: {p}"
                    )

            return p

        return p

    return _validator


# ------------------------------
# Reuse argparse validator manually
# ------------------------------
def apply_validator(validator, path_str):
    """
    Apply an argparse-style validator manually.
    Useful when validating paths from config/YAML (not from CLI).
    """
    try:
        return validator(path_str)
    except argparse.ArgumentTypeError as e:
        raise ValueError(str(e))




def validate_plink_prefix(prefix: str):
    prefix = Path(prefix)

    bed = prefix.with_suffix(".bed")
    bim = prefix.with_suffix(".bim")
    fam = prefix.with_suffix(".fam")

    missing = [p for p in (bed, bim, fam) if not p.exists()]
    if missing:
        missing_list = "\n".join([f"  - {str(m)}" for m in missing])
        raise FileNotFoundError(
            f"PLINK reference panel prefix is incomplete.\n"
            f"Missing files:\n{missing_list}"
        )
    return prefix




def validate_alphanumeric(value: str) -> str:
    """
    Generalized validator:
    Allows only letters, digits, and underscores.
    No spaces or punctuation.
    Safe for filenames and all GWAS sample IDs.
    """
    import argparse
    import re

    if value is None:
        raise argparse.ArgumentTypeError("Value cannot be empty.")

    v = value.strip()

    # Allowed: A–Z, a–z, 0–9, underscore
    if not re.match(r'^[A-Za-z0-9_]+$', v):
        raise argparse.ArgumentTypeError(
            f"Invalid value '{value}': must contain only letters, digits, and underscores."
        )

    return v



import glob
from pathlib import Path
def validate_prefix_files(prefix: str, suffixes):
    """
    General-purpose validator:
    Accepts prefix + list of suffixes or wildcard patterns.
    Example suffixes:
        [".bed", ".bim", ".fam"]
        ["*.vcf.gz"]
        ["*_scores.txt.gz"]
    """
    prefix = Path(prefix)

    if isinstance(suffixes, str):
        suffixes = [suffixes]

    missing = []

    for suf in suffixes:

        # 1️⃣ Wildcard patterns: "*.vcf.gz", "*txt"
        if "*" in suf:
            pattern = str(prefix) + suf
            files = [Path(f) for f in glob.glob(pattern)]

        # 2️⃣ Direct suffixes: ".bed", ".bim", ".fam"
        else:
            # DO NOT use .with_suffix() → breaks prefixes with dots
            f = Path(str(prefix) + suf)
            files = [f] if f.exists() else []

        # Validate existence + non-empty
        valid = [f for f in files if f.exists() and f.stat().st_size > 0]

        if not valid:
            missing.append(str(prefix) + suf)

    if missing:
        raise FileNotFoundError("Missing required files:\n" + "\n".join(missing))

    return str(prefix)


def detect_total_memory_gb():
    """
    Safely detect total physical memory (RAM) in GB.
    Works on macOS, Linux, and Windows — never fails.
    """

    # Try psutil (best, cross-platform)
    try:
        import psutil
        return psutil.virtual_memory().total // (1024**3)
    except Exception:
        pass

    # Linux fallback: /proc/meminfo
    if os.path.exists("/proc/meminfo"):
        try:
            with open("/proc/meminfo") as f:
                for line in f:
                    if line.startswith("MemTotal:"):
                        kb = int(line.split()[1])
                        return (kb * 1024) // (1024**3)
        except Exception:
            pass

    # macOS fallback: sysctl
    if sys.platform == "darwin":
        try:
            out = subprocess.check_output(["sysctl", "-n", "hw.memsize"])
            return int(out.strip()) // (1024**3)
        except Exception:
            pass

    # Windows fallback (psutil usually works here)
    if sys.platform == "win32":
        try:
            import psutil
            return psutil.virtual_memory().total // (1024**3)
        except Exception:
            pass

    # Last safe fallback
    return 4  # assume at least 4GB
